# -*- coding: utf-8 -*-
"""eda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GwK3xaJUwISV6wpJbnKvBzHRbW_b77HC
"""

pip install pandas

import pandas as pd

df = pd.read_csv("/content/Telco-Customer-Churn.csv")
print(df.head())
print(df.info())

print(df.isnull().sum())

# Identify rows where 'TotalCharges' contains empty strings (invalid entries)
df[df['TotalCharges'].str.strip() == '']

# Convert 'TotalCharges' to numeric
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Check missing again
print(df.isnull().sum())

# Drop missing values or impute
df.dropna(inplace=True)

# Check missing again
print(df.isnull().sum())

# Drop missing values or impute
df.dropna(inplace=True)

# Drop customerID as it's just an identifier
df.drop('customerID', axis=1, inplace=True)

# LABEL ENCODING:Encode binary columns
binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']
for col in binary_cols:
    df[col] = df[col].map({'Yes': 1, 'No': 0, 'Female': 0, 'Male': 1})

print(df.head())

# One-hot encode multi-class categorical features
df = pd.get_dummies(df, drop_first=False)
print(df.head())  # or df.columns

print([col for col in df.columns if col.startswith('TechSupport_')])

#feature scaling
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df[['tenure', 'MonthlyCharges', 'TotalCharges']] = scaler.fit_transform(df[['tenure', 'MonthlyCharges', 'TotalCharges']])

print(df.head())

#outlier detection
import numpy as np

z_scores = np.abs((df[['tenure', 'MonthlyCharges', 'TotalCharges']] - df[['tenure', 'MonthlyCharges', 'TotalCharges']].mean()) / df[['tenure', 'MonthlyCharges', 'TotalCharges']].std())
print((z_scores > 3).sum())  # Count outliers per feature

#max absolute Z-score per feature:
print(z_scores.max())

"""| Column           | Max Z-score | Interpretation                                                              |
| ---------------- | ----------- | --------------------------------------------------------------------------- |
| `tenure`         | **1.61**    | The most extreme value is only \~1.6 standard deviations away from the mean |
| `MonthlyCharges` | **1.79**    | No value is very far from the average                                       |
| `TotalCharges`   | **2.82**    | This is the closest to the 3.0 threshold for being an outlier               |

"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(data=df, x='Churn')
plt.title("Churn Distribution")
plt.show()

# Example: Churn vs Contract
sns.countplot(data=df, x='Contract_One year', hue='Churn')
plt.title("Churn by One-Year Contract")
plt.show()

df[['tenure', 'MonthlyCharges', 'TotalCharges']].hist(bins=30, figsize=(12, 6))
plt.tight_layout()
plt.show()

plt.figure(figsize=(12,8))
sns.heatmap(df.corr(), cmap="coolwarm", annot=False)
plt.title("Correlation Matrix")
plt.show()

"""| Feature                                | Observation                              | Churn Risk |
| -------------------------------------- | ---------------------------------------- | ---------- |
| `Contract`                             | Longer contracts reduce churn            | üîΩ Lower   |
| `tenure`                               | New customers churn more                 | üîº Higher  |
| `TotalCharges`                         | Lower charges (new users) = higher churn | üîº Higher  |
| `TechSupport`, `OnlineSecurity`        | Absence increases churn                  | üîº Higher  |
| `PaperlessBilling`, `Electronic check` | Positively correlated with churn         | üîº Higher  |


| Feature                          | Effect on Churn   | Recommendation / Use                    |
| -------------------------------- | ----------------- | --------------------------------------- |
| `Contract_One year`              | Reduces churn ‚úÖ   | Encourage longer-term contracts         |
| `Contract_Month-to-month`        | Increases churn ‚ùå | Target with loyalty offers              |
| `tenure` (low values)            | Increases churn ‚ùå | Focus on new customer onboarding        |
| `TotalCharges` (low)             | Increases churn ‚ùå | Monitor short-term users                |
| `TechSupport_Yes`                | Reduces churn ‚úÖ   | Offer support to increase retention     |
| `OnlineSecurity_Yes`             | Reduces churn ‚úÖ   | Promote value-added services            |
| `PaymentMethod_Electronic check` | Increases churn ‚ùå | Encourage switching to auto-pay options |

"""



# Split the data into features and target
X = df.drop('Churn', axis=1)
y = df['Churn']

# Train-test split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

print("Training data shape:", X_train.shape)
print("Test data shape:", X_test.shape)

"""# -------------------------
# RANDOM FOREST CLASSIFIER
# -------------------------
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score, roc_curve

# Initialize model
rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)

# Train
rf_model.fit(X_train, y_train)

# Predict
y_pred_rf = rf_model.predict(X_test)
y_proba_rf = rf_model.predict_proba(X_test)[:, 1]  # for ROC AUC

# Evaluation Metrics
print("üéØ Random Forest Metrics")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Precision:", precision_score(y_test, y_pred_rf))
print("Recall:", recall_score(y_test, y_pred_rf))
print("F1 Score:", f1_score(y_test, y_pred_rf))
print("ROC-AUC Score:", roc_auc_score(y_test, y_proba_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

# Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Blues')
plt.title("Random Forest Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Feature Importance Plot
import pandas as pd
feat_importance = pd.Series(rf_model.feature_importances_, index=X.columns)
feat_importance.nlargest(15).sort_values().plot(kind='barh', figsize=(10, 6), color='teal')
plt.title("Top 15 Important Features (Random Forest)")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba_rf)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label="Random Forest", color="green")
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Random Forest")
plt.legend()
plt.grid()
plt.show()

"""# -------------------------
# DECISION TREE CLASSIFIER
# -------------------------
"""

from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)
dt_model.fit(X_train, y_train)

y_pred_dt = dt_model.predict(X_test)
y_proba_dt = dt_model.predict_proba(X_test)[:, 1]

print("üå≥ Decision Tree Metrics")
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Precision:", precision_score(y_test, y_pred_dt))
print("Recall:", recall_score(y_test, y_pred_dt))
print("F1 Score:", f1_score(y_test, y_pred_dt))
print("ROC-AUC Score:", roc_auc_score(y_test, y_proba_dt))
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))

import joblib

# Save columns
joblib.dump(X.columns.tolist(), "columns.pkl")

import joblib

# Save Random Forest model
joblib.dump(rf_model, 'random_forest_model.pkl')

import joblib

# Save Decision Tree model
joblib.dump(dt_model, 'decision_tree_model.pkl')

# Load Random Forest model
rf_loaded = joblib.load('random_forest_model.pkl')

# Load Decision Tree model
dt_loaded = joblib.load('decision_tree_model.pkl')

# Predict using loaded model
rf_loaded.predict(X_test)
#o/p: 1 means will churn & 0 means will not churn





# ROC Curve
fpr_dt, tpr_dt, _ = roc_curve(y_test, y_proba_dt)
plt.figure(figsize=(8, 6))
plt.plot(fpr_dt, tpr_dt, label="Decision Tree", color="orange")
plt.plot(fpr, tpr, label="Random Forest", color="green")
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Decision Tree vs Random Forest")
plt.legend()
plt.grid()
plt.show()

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 150, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_rf.fit(X_train, y_train)

print("Best Parameters:", grid_rf.best_params_)
print("Best Score on CV:", grid_rf.best_score_)









